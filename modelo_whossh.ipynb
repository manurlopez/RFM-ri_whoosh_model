{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo realizado con *whoosh* para o TFM **\"Estimación automática de signos de depresión a partir de análises de texto.\"** do Máster universitario en tecnoloxías de análise de datos masivos: Big Data no curso académico 2019/2020\n",
    "\n",
    "## Autor: Manuel Ramón Varela López"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ver **Readme** para as instruccións de uso. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pasos previos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Comezamos cos datos de configuración do notebook.** \n",
    "    - Indicamos o ficherio JSON cas preguntas e respostas do test BDI.\n",
    "    - Indicamos o directorio onde se gardan os documentos XML cas publicacións dos usuarios.\n",
    "    - Indicamos o ficheiro que contén os resultados reais.\n",
    "    - Indicamos o directorio no que se crearán os índices para cada usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_file = \"inquerito.json\"\n",
    "dir_corpus = 'corpus'\n",
    "file_real_results = 'Depression Questionnaires_anon.txt'\n",
    "dir_index = 'indexes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Importamos as librerías**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from whoosh.fields import Schema, TEXT, KEYWORD\n",
    "from whoosh import index\n",
    "from whoosh.writing import BufferedWriter\n",
    "from whoosh.qparser import QueryParser,OrGroup\n",
    "from whoosh.query import Term\n",
    "from whoosh.analysis import StopFilter, RegexTokenizer\n",
    "from whoosh.lang.porter import stem\n",
    "from whoosh.lang.morph_en import variations\n",
    "from whoosh.lang.wordnet import Thesaurus\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from whoosh.scoring import TF_IDF\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Configuración**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comezo do script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Cargamos as preguntas.** \n",
    "    - Cargamos as preguntas do test BDI do arquivo indicado ao comezo do script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(questions_file) as json_file:\n",
    "    questions = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Consultamos o número de xml cás publicacións de Reddit**.\n",
    "    - Collemos todos os arquivos XML (un para cada usuario) no directorio na que se gardan estes arquivos indicado ao comezo do script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_files = [f for f in os.listdir(dir_corpus) if os.path.isfile(os.path.join(dir_corpus, f))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Lemos os arquivos XML**.\n",
    "    - Preocesamos esos arquivos e gardámola información toda nun dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for file in corpus_files:\n",
    "    dataElement = {}\n",
    "    writings = []\n",
    "    tree = ET.parse(dir_corpus + os.path.sep + file)\n",
    "    root = tree.getroot()\n",
    "    for child in root:\n",
    "        if child.tag == 'ID':\n",
    "            dataElement['id']=child.text\n",
    "        elif child.tag=='WRITING':\n",
    "            writing = {}\n",
    "            for wriIter in child:\n",
    "                if wriIter.tag == 'TITLE':\n",
    "                    writing['title']=wriIter.text\n",
    "                elif wriIter.tag == 'DATE':\n",
    "                    writing['date']=wriIter.text\n",
    "                elif wriIter.tag == 'INFO':\n",
    "                    writing['info']=wriIter.text\n",
    "                elif wriIter.tag == 'TEXT':\n",
    "                    writing['text']=wriIter.text\n",
    "            writings.append(writing)\n",
    "    dataElement['corpus'] = writings\n",
    "    data.append(dataElement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Creamos o esquema polo que se rixirán os documentos do índice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = Schema(text=TEXT(stored=True),subject=KEYWORD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Eliminamos a carpeta que conteñen os índices e volvémola crear**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(dir_index):\n",
    "    shutil.rmtree(dir_index)\n",
    "\n",
    "os.mkdir(dir_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Función que engade para un usuario as súas publicacións no seu correspondente índice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Métese a información no índice\n",
    "def insert_data_user(ix,user):\n",
    "\n",
    "    #Abrimos o writer para escribir\n",
    "    writer = BufferedWriter(ix, period=120, limit=10)\n",
    "\n",
    "    #Recorremos todos os usuarios\n",
    "    text = ''\n",
    "    identificador = user['id']\n",
    "    #Imos gardando cada documento no índice\n",
    "    for doc in user['corpus']:\n",
    "        text = doc['title'] + ' ' + doc['text']\n",
    "        writer.add_document(text=text,subject=identificador)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Función que crea un índice para un usuario**\n",
    "    - Crea un índice para o usuario que se lle pasa por parámetro.\n",
    "    - Engade as publicacións do usuario ao índice.\n",
    "    - Devolve o índice creado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función que crea o índice\n",
    "def create_index(user):\n",
    "\n",
    "    #Se existe do usuario borramolo\n",
    "    name_index = user['id']\n",
    "    aux_dir = dir_index + os.sep + name_index\n",
    "    if os.path.exists(aux_dir):\n",
    "        shutil.rmtree(aux_dir)\n",
    "\n",
    "    #Creamos a carpeta para o indice\n",
    "    os.mkdir(aux_dir)\n",
    "    index.create_in(aux_dir, schema)\n",
    "\n",
    "    #Abrimos o indice\n",
    "    ix = index.open_dir(aux_dir)\n",
    "\n",
    "    insert_data_user(ix,user)\n",
    "\n",
    "    return ix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Collemos o tokenizador para separar palabras, o filtro para as \"stop-words\" e cargamos os sinónimos.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexTokenizer()\n",
    "stopper = StopFilter()\n",
    "#Collemos os sinonimos e metémolos en memoria\n",
    "f = open(\"utils/wn_s.pl\")\n",
    "thesaurus = Thesaurus.from_file(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Función que para cada palabra a transforma en minúsculas todas as letras.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LowercaseFilter(tokens):\n",
    "    for t in tokens:\n",
    "        t.text = t.text.lower()\n",
    "        yield t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Función que recibe un array de palabras e engade a este array as palabras do array sen as terminacións**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_terminations(words):\n",
    "    words_aux = []\n",
    "    for w in words:\n",
    "        aux = stem(w)\n",
    "        if (aux not in words) and (aux not in words_aux):\n",
    "            words_aux.append(aux)\n",
    "    words = words + words_aux\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Función que recibe un array de palabras e engade a ese array as palabras derivadas de cada unha das palabras.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_terminations(words):\n",
    "    words_aux = []\n",
    "    for w in words:\n",
    "        aux = variations(w)\n",
    "        for aux2 in aux:\n",
    "            if (aux2 not in words) and (aux2 not in words_aux):\n",
    "                words_aux.append(aux2)\n",
    "    words = words + words_aux\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Función que transforma un array de palabras nunha cadea de texto con todas as palabras.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_to_string(words):\n",
    "    text = \"\"\n",
    "    for w in words:\n",
    "        text = text + \" \" + w\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Función que constrúe a consulta a partir dunha cadea de texto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_query(phrase):\n",
    "    \n",
    "    #Tokenizamos, poñemolas en minuscula e quitamos as stopwords -> Esto facemolo sempre\n",
    "    words = []\n",
    "    for token in stopper(LowercaseFilter(tokenizer(phrase))):\n",
    "        words.append(token.text)\n",
    "\n",
    "    #Agora imos quitarlle as terminacións e añadimolas\n",
    "    words = delete_terminations(words)\n",
    "\n",
    "    #Engadimos as terminacións\n",
    "    words = add_terminations(words)\n",
    "\n",
    "    return vector_to_string(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Devolve o texto da consulta collendo a pregunta a resposta máis alta**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query(question):\n",
    "    answers = question['answers']\n",
    "    answer = answers[len(answers)-1]\n",
    "    answer_text = answer['answer_text']\n",
    "    return question['question_text'] + ' ' + answer_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Devolve o número de documentos para unha consulta.**\n",
    "    - Recibe o índice no que se encontra o corpus.\n",
    "    - Recibe tamén a consulta.\n",
    "    - Realizase unha consulta ao índice disxuntiva.\n",
    "    - Devólvese o número de documentos recuperados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_puntuation(ix,query):\n",
    "    with ix.searcher() as s:\n",
    "        qp = QueryParser(\"text\", schema=schema,group=OrGroup)\n",
    "        allow_q = Term(\"subject\", subject)\n",
    "        \n",
    "        #Facemos unica busqueda\n",
    "        q = qp.parse(query)\n",
    "        res = s.search(q,filter=allow_q,limit=None)\n",
    "        numero_ducumentos = len(res)\n",
    "\n",
    "        return numero_ducumentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Creamos un DataFrame para gardar os resultados.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recorremos todas as preguntas\n",
    "d = ['subject']\n",
    "for question in questions:\n",
    "    d.append(question['question_number'])\n",
    "\n",
    "#Creamos os dataframes para as medidas\n",
    "results = pd.DataFrame(columns=d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Realizamos o cálculo de porcentaxe de documentos recuperados para cada usuario e pregunta**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recorremos todos os usuarios\n",
    "for user in data:\n",
    "    \n",
    "    #Collemos o id do usuario\n",
    "    subject = user['id']\n",
    "    \n",
    "    #Collemos o número de publicacións do usuario\n",
    "    len_corpus = len(user['corpus'])\n",
    "    \n",
    "    #Creamos o índice para o usuario e engadimos todas as publicacións.\n",
    "    ix = create_index(user)\n",
    "    \n",
    "    #Imos gardando as medidas para cada usuario\n",
    "    subject_res = {'subject':subject}\n",
    "    \n",
    "    #Collemos as preguntas\n",
    "    for question in questions:\n",
    "        \n",
    "        #Collemos o texto o número da pregunta\n",
    "        question_number = question['question_number']\n",
    "        \n",
    "        #Collemos as palabras da consulta\n",
    "        query_aux =  get_query(question)\n",
    "        \n",
    "        #Cosntruimos a consulta\n",
    "        query = build_query(query_aux)\n",
    "        \n",
    "        #Calculamos as puntuacións\n",
    "        question_puntuation = get_puntuation(ix,query)\n",
    "        \n",
    "        #Calculamos as porcentaxes\n",
    "        if len_corpus > 0:\n",
    "            score = question_puntuation / len_corpus\n",
    "\n",
    "        subject_res[question_number] = score\n",
    "        \n",
    "    #Gardamos os scores para cada usuario\n",
    "    results = results.append(subject_res,ignore_index=True)\n",
    "    \n",
    "    #Indicamos que un usuario rematou de ler\n",
    "    print(\"User \" + subject + \" finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Mostramos os resultados obtidos**\n",
    "    - Indicamos que a columna \"subject\" é índice do DataFrame.\n",
    "    - Mostramos o data frame, para cada usuario e pregunta indicamos a similitude entre a pregunta e a resposta máis alta e o corpus do usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=results.set_index('subject')\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Funcións para transformar os resultados obtidos as respostas seleccionadas**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preguntas4 = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,17,19,20,21]\n",
    "preguntas7 = [16,18]\n",
    "\n",
    "def p4(value):\n",
    "    if(value>=1):\n",
    "        return 3\n",
    "    elif(value>=0.66):\n",
    "        return 2\n",
    "    elif(value>=0.33):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def p7(value):\n",
    "    if(value>=1):\n",
    "        return '3b'\n",
    "    elif(value>=0.83):\n",
    "        return '3a'\n",
    "    elif(value>=0.67):\n",
    "        return '2b'\n",
    "    elif(value>=0.5):\n",
    "        return '2a'\n",
    "    elif(value>=0.33):\n",
    "        return '1b'\n",
    "    elif(value>=0.17):\n",
    "        return '1a'\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Tranformarmos os resultados obtidos en respostas cubertas.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in preguntas4:\n",
    "    results[i] = results[i].map(p4)\n",
    "for i in preguntas7:\n",
    "    results[i] = results[i].map(p7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Mostramos as respostas de cada usuario para cada unha das preguntas.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Cargamos os datos reais e gardámolos nun DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_results = pd.read_csv(file_real_results,index_col=False,header=None,sep='\\t')\n",
    "real_results.columns = d\n",
    "real_results=real_results.set_index('subject')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Mostramos os resultados reais**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(real_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Funcións necesarias para o calculo dos resultados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necesitamos esta función porque algunhas preguntas teñen letra\n",
    "def transform(answer):\n",
    "    if(type(answer)!=str):\n",
    "        return answer\n",
    "    elif len(answer) == 1:\n",
    "        return int(answer)\n",
    "    elif len(answer) == 2:\n",
    "        return int(answer[0])\n",
    "\n",
    "def dhcl_score(real,estimed):\n",
    "    if (real <= 9) and (estimed<=9):\n",
    "        return 1\n",
    "    elif (real>29) and (estimed>29):\n",
    "        return 1\n",
    "    elif (real>9) and (real<=18) and (estimed>9) and (estimed<=18):\n",
    "        return 1\n",
    "    elif (real>18) and (real<=29) and (estimed>18) and (estimed<=29):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Calculamos as 4 medidas de avaliación descritas no TFM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mad = 3\n",
    "score_array = []\n",
    "\n",
    "#Recorremos todos os usuarios\n",
    "for identificador in real_results.index.values:\n",
    "    hits = 0\n",
    "    crs = 0\n",
    "    scr_real = 0\n",
    "    scr_stm = 0\n",
    "    #Recorremos todas as preguntas\n",
    "    for question in questions:\n",
    "        q = question['question_number']\n",
    "        real_a = real_results.loc[identificador,q]\n",
    "        estimated_a = results.loc[identificador,q]\n",
    "\n",
    "        #Contamos as pregutnas acertadas\n",
    "        if real_a == estimated_a:\n",
    "            hits = hits + 1\n",
    "\n",
    "        #Contamos camo de cerca estamos\n",
    "        crs_aux = (mad - abs(transform(real_a)-transform(estimated_a)))/mad\n",
    "        crs = crs + crs_aux\n",
    "\n",
    "        #Calculamos os valores de depresion\n",
    "        scr_real = scr_real + transform(real_a)\n",
    "        scr_stm = scr_stm + transform(estimated_a)\n",
    "\n",
    "    #Calculamos o porcentaxe de preguntas acertadas\n",
    "    hit_score_aux = hits / len(questions)\n",
    "    cls_score_aux = crs / len(questions)\n",
    "    dl = (63 - abs(scr_real - scr_stm))/63\n",
    "    dhcl = dhcl_score(scr_real,scr_stm)\n",
    "    score_array.append({'subject':identificador,\n",
    "                        'hit rate score':hit_score_aux,\n",
    "                        'closeness rate score':cls_score_aux,\n",
    "                        'real score':scr_real,\n",
    "                        'estimated_score':scr_stm,\n",
    "                        'dl':dl,\n",
    "                        'dchr':dhcl})\n",
    "\n",
    "\n",
    "score = pd.DataFrame(score_array)\n",
    "score = score.set_index('subject')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Mostramos as 4 medidas de avaliación para cada un dos suxeitos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Mostramos as medidas de avaliación do modelo proposto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(score[['hit rate score','closeness rate score','dl','dchr']].describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
